"""
Bridge - Backward-Compatible Adapter for V2 Type System
=========================================================

Provides adapter functions that let the V2 type system work with
the existing training pipeline without rewriting the training loop.

Usage:
    from mlx_grpo.trainer.type_system_v2.bridge import (
        create_v2_coordinator,
        v2_reward_adapter,
    )

    coordinator = create_v2_coordinator(tokenizer)
    reward_func = v2_reward_adapter(coordinator)

    # Use as standard reward function
    scores = reward_func(prompts, completions, answers, types=types)
"""

from __future__ import annotations

import logging
from typing import Any, Callable, Optional, TYPE_CHECKING

from .coordinator import TypeCoordinator, auto_register_builtin_types, normalize_type
from .events import EventBus

if TYPE_CHECKING:
    from transformers import PreTrainedTokenizer

__all__ = [
    "create_v2_coordinator",
    "v2_reward_adapter",
    "v2_type_normalizer",
]

logger = logging.getLogger(__name__)


def create_v2_coordinator(
    tokenizer: PreTrainedTokenizer,
    event_bus: Optional[EventBus] = None,
) -> TypeCoordinator:
    """Factory: create a fully-configured TypeCoordinator.

    Registers all built-in types (tool_call, mcq, general_qna).

    Args:
        tokenizer: HuggingFace tokenizer
        event_bus: Optional shared event bus

    Returns:
        Configured TypeCoordinator with all types registered
    """
    coordinator = TypeCoordinator(event_bus=event_bus)
    auto_register_builtin_types(coordinator, tokenizer)
    return coordinator


def v2_reward_adapter(
    coordinator: TypeCoordinator,
) -> Callable:
    """Create a reward function matching the old pipeline signature.

    Returns a callable with signature:
        func(prompts, completions, answers, types=None) -> List[float]

    Dispatches to type-specific rewards based on the types parameter.

    Args:
        coordinator: Configured TypeCoordinator

    Returns:
        Reward function compatible with existing training pipeline
    """

    def reward_func(
        prompts: list[str],
        completions: list[str],
        answers: list[str],
        types: Optional[list[Any]] = None,
    ) -> list[float]:
        """Type-dispatched reward function.

        Groups samples by type and dispatches to appropriate reward.
        """
        # noqa: this function is generated by v2_reward_adapter
        n = len(completions)

        if types is None:
            types = [None] * n

        # Normalize types
        normalized = [normalize_type(t) for t in types]

        # Group by type for batch processing
        type_groups: dict[str, list[int]] = {}
        for i, t in enumerate(normalized):
            type_groups.setdefault(t, []).append(i)

        # Compute rewards per type
        scores = [0.0] * n

        for type_name, indices in type_groups.items():
            try:
                reward = coordinator.get_reward(type_name)
            except KeyError:
                # Unknown type - try default
                try:
                    reward = coordinator.get_reward("general_qna")
                except KeyError:
                    logger.warning("No reward for type '%s', scoring 0.0", type_name)
                    continue

            # Gather batch for this type
            batch_prompts = [prompts[i] for i in indices]
            batch_completions = [completions[i] for i in indices]
            batch_answers = [answers[i] for i in indices]
            batch_types = [types[i] for i in indices]

            # Compute rewards
            batch_scores = reward.compute(
                batch_prompts, batch_completions, batch_answers, batch_types
            )

            # Scatter back
            for idx, score in zip(indices, batch_scores):
                scores[idx] = score

        return scores

    # Mark as v2 adapter to prevent double-prepending in train_grpo()
    reward_func._is_v2_adapter = True  # type: ignore[attr-defined]
    reward_func.__name__ = "v2_type_dispatched_reward"
    return reward_func


def v2_type_normalizer(raw_type: str | dict | None) -> str:
    """Normalize type using v2 system.

    Convenience wrapper around coordinator.normalize_type().

    Args:
        raw_type: Raw type string, dict, or None

    Returns:
        Canonical type name (tool_call, mcq, general_qna)
    """
    return normalize_type(raw_type)
